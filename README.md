# Autonomous Drone Inspection of NIST Buckets
Autonomous search, inspection and detection of National Institute of Standards and Technology (NIST) buckets indoors using ModalAI's VOXL M500 drone.

![GROUND_alignment_apriltag](https://github.com/amashry/nist-autonomous-inspection/assets/98168605/37931dee-5e8d-4bd7-ab9d-2cd8cfe3c257)
*Figure 1: Illustration of the GROUND bucket alignment.*

## Overview
This project employs a drone to autonomously inspect buckets in an unknown environment. The buckets are organized in various alignments, each represented by a unique AprilTag. The drone searches a predefined area of interest and upon detecting an AprilTag, it loads the inspection routine corresponding to that tag's bucket alignment. During inspection, the drone utilizes a [custom YOLOv5 model running on TFLITE server](https://github.com/amashry/CustomYOLOv5-NIST-Buckets-VOXL-tflite) onboard on the drone for detecting what's inside the buckets. 

## Introduction
This project's goal was to create software that uses a drone's onboard camera to localize a bucket configuration representing indoor debris in a disaster scenario, aiming to automate the task of a human pilot navigating around these obstacles. By using AprilTag, we simplified the complex task of debris detection into detecting and localizing an AprilTag on the bucket configuration. The drone was then instructed to move to certain predefined poses where its tracking camera could look into the buckets.

A JSON file containing these poses was generated by a program, "generate_waypoints", for each bucket configuration. The localization of the bucket configuration in the local inertial frame was performed using the Pose of the AprilTag in the drone's camera frame and the drone's own pose estimate in the local inertial frame.

By chaining several homogeneous transformations, we obtained the desired pose of the drone in the local inertial frame and fed this information into the drone's autopilot. Smooth drone movement was achieved by controlling yaw rates rather than absolute yaw angles, which reduced image blur from the tracking camera.

This project is primarily built around the ROS (Robot Operating System) framework, and uses MAVROS for interfacing with the PX4 autopilot. The inspection routines are defined as a series of waypoints and movements around the bucket alignments.

## Prerequisites and Dependencies

This project depends on a number of libraries and frameworks:

1. ROS (Robot Operating System): It is the main framework that ties everything together. ROS Melodic was used for this project, but other versions should also work.
2. MAVROS: This is a ROS package that provides a bridge between ROS and the drone's autopilot. It allows us to send commands to the drone and receive telemetry data from it using ROS.
3. Eigen: This is a high-level C++ library for linear algebra, matrix and vector operations, numerical solvers and more. It is used extensively in this project to perform matrix operations and transformations.
4. AprilTag: This is a visual fiducial system popular for robotics research.
5. [JSON for Modern C++](https://github.com/nlohmann/json): A C++ library for reading and writing JSON files. It is used to read the predefined waypoints from JSON files. 

The project also requires a drone equipped with a tracking camera, and an AprilTag attached to the bucket configuration as shown in figure 1 above. 

## Installation and Usage

The installation process begins by cloning the repository and installing the dependencies.

The usage process involves setting up the drone and the ROS environment, launching the ROS node for this project, and then taking off with the drone. Once the drone spots an AprilTag, switch the drone to `offboard` mode, then it will automatically start navigating towards the predefined waypoints. The process can be monitored through the ROS logs or a separate ROS node that visualizes the drone's trajectory.

To use the software, first clone the repository and navigate into it:

```bash
git clone https://github.com/amashry/nist-autonomous-inspection.git
cd nist_autonomous_inspection
```

After installing the dependencies, build the project:

```bash
catkin_make
```
Then, source the workspace's setup file and launch the project using the launch file. 
```bash
source devel/setup.bash
```
You can now launch the ROS node:

```bash
roslaunch launch/indoor.launch
```

## Project Structure

The project is organized in the following directories:

- `src`: Contains the main ROS node (`NFRA_node`) for controlling the drone, and utilities for various tasks like waypoint management, reading bucket configurations, etc.
- `include`: Contains header files for the utilities.
- `launch`: Contains ROS launch files for starting the project.

The `NFRA_node` is the central component of the project, and coordinates the bucket inspection process. It uses MAVROS services and publishes to MAVROS topics to command the drone.

## Significant files
Two main files, "project.cpp" and "utils.cpp", along with a few supporting files and libraries, are responsible for most of the project's implementation. The "project.cpp" file is the main file, whereas "utils.cpp" contains utility functions.

### project.cpp
This file contains the main logic of the software, starting with initializing ROS node, Subscribers, and Publishers. It then reads JSON files containing the waypoint offsets for each bucket. When the drone spots an AprilTag, it loads the corresponding JSON file based on the AprilTag's ID and begins navigating to each waypoint.

For each waypoint, the drone calculates its desired pose in the local inertial frame, which is subsequently published to the "/mavros/setpoint_raw/local" rostopic. The drone continues towards its goal until it approximately reaches the desired position and yaw angle, which is determined using the "drone_is_approximately_at_offset" function from the "utils.cpp" file.

This file also handles the logic for when to transition from one bucket to the next. The desired tolerances are defined as global variables at the top of the "project.cpp" file and can be easily modified to suit the mission requirements.

### utils.cpp
This file contains several utility functions that assist the main program. The "determine_yaw_rate" function calculates the difference in yaw angle between the drone's current pose and its desired pose. This allows us to control the drone's yaw rate for smoother navigation and better image quality.

The "drone_is_approximately_at_offset" function checks if the drone's configuration, defined by its yaw angle and position, is within some predefined tolerance of the desired drone configuration. This helps to determine whether the drone has reached its destination (bucket location) or not.

## Bucket Configurations

Each bucket alignment is represented by a unique AprilTag. The bucket configurations are stored in JSON files inside the `config` directory, and handled by the `BucketConfiguration` class. Each configuration specifies the name of the bucket alignment and an offset matrix.

For example, one of the buckets in the "GROUND" bucket alignment is represented by the following configuration:

```json
{
    "name": "bucket1a",
    "offset": [
        -0.626168, -0.779510, 0.024069, 0.504049, 
        -0.779751, 0.625586, -0.025125, 0.206124, 
        0.004528, -0.034500, -0.999546, -0.42000, 
        0.000000, 0.000000, 0.000000, 1.000000
    ]
}
```

## Mathematical Description

![frames_illustration](https://github.com/amashry/nist-autonomous-inspection/assets/98168605/e53c3487-fe0f-4236-94f1-3c9a820fd357)
*Figure 2: Illustration of the different reference frames. (figure by [Zach Bortoff](https://github.com/zborffs))*

This project utilizes the concept of homogeneous transformations to represent the drone's pose, the AprilTag's pose, and the static transformation between the drone’s camera and body frames. A homogeneous transformation is a 4x4 matrix that encodes a 3D rotation and translation. It allows complex transformations such as rotations and translations to be performed using matrix multiplication. In our context, these transformations represent poses in different frames of reference:

- ${}^{I}H^{B}$: The drone's estimate of its own pose relative to the local inertial frame. It is an output of the drone's internal VIO system and changes as the drone moves.
- ${}^{C}H^{A}$: The drone's perception of the AprilTag pose in the camera frame. This pose estimate is obtained by getting subscribing to the ROS topic being published by the tag_detection ROS package, which gives the tag_pose in the camera frame. 
- ${}^{B}H^{C}$: The static transformation between the drone’s body frame and camera frame. This is a fixed transform because the camera is rigidly attached to the drone at a 45 degrees downward tilt.

To compute the pose of the AprilTag in the local inertial frame, we chain these transformations together. Chaining involves multiplying these transformations in the order of frames from target to source:

$${}^{I}H^{A} = {}^{I}H^{B} \ \ {}^{B}H^{C} \ \ {}^{C}H^{A}$$

This operation effectively maps the pose of the AprilTag from the camera frame to the local inertial frame, considering the drone's pose and the fixed camera-body transformation.

The `generate_waypoints` program simplifies this procedure by transforming the AprilTag’s pose from the camera frame directly to the drone’s body-frame. As mentioned earlier, this program is used to generate the desired offsets for the drone to be able to see inside the buckets. 

$${}^{B}H^{A} = {}^{B}H^{C} \ \ {}^{C}H^{A}$$

For each detected AprilTag, a corresponding JSON file provides the offset configurations between the AprilTag and a desired drone pose ${}^{O}H^{A}$. This offset is crucial for determining the drone's target position and is incorporated into the computation of the **published** desired drone pose in the inertial frame:

$${}^{I}H^{O} = {}^{I}H^{B} \ \ {}^{B}H^{C} \ \ {}^{C}H^{A} \ \ ({}^{O}H^{A})^{-1}$$

The computed ${}^{I}H^{O}$ represents the desired drone pose in the inertial frame, with ${}^{O}H^{A}$ as the desired offset from the detected AprilTag.

The translational components of ${}^{I}H^{O}$ are published to the rostopic _“/mavros/setpoint_raw/local”_, guiding the drone’s autopilot to the target position. The orientation change, specifically the yaw difference between the drone's current pose ${}^{I}H^{B}$ and the desired pose ${}^{I}H^{O}$, is computed using the function `determine_yaw_rate` in `utils.cpp`. This calculated yaw rate is used to steer the drone towards the desired pose while maintaining the correct orientation.

## Demonstration Video

https://github.com/amashry/nist-autonomous-inspection/assets/98168605/b1151bd4-eadf-46df-82b4-0116a37cdbee


*Figure 3: Demonstration video of the autonomous inspection of the "GROUND" bucket alignment by the drone.*

## TODO

- Implement a search routine for the drone to navigate a predefined area of interest.
- Allow the drone to switch between the search and inspection routines dynamically.
- Integrate adaptive path planning for the drone based on the detection.

## Improvements 
1. calculate yaw angle at current position to publish search waypoints at the correct YAW Angle
2. Optimize the search and inspection time.
3. Fix the case when it's hovering and cannot see the apriltag because it keeps yawing in place

## BIG IMPROVEMENTS 
1. BIG IMPROVEMENT IS TO GET THE APRILTAG OUT OF THE INSPECTION ROUTINE !!!! THINK OF SAVING THE APRIL TAG POSE IN THE INERTIAL FRAME ONCE IT"S BEEN SEEN AND THEN CALCULATE THE PUBLISHED OFFSET BASED ON THE DESIRED OFFSET LOADED FROM CONFIG FILE AND APRIL TAG POSE IN THE INERTIAL FRAME 
2. INTEGRATE THE DETECTION OF THE BUCKETS IN THE LOOP TO GO TO THE NEXT BUCKET ONCE THE BUCKET IS IN GOOD FRAME AND COULD CLEARLY BE SEEN 


## Acknowledgments

A big thank you to [Zach Bortoff](https://github.com/zborffs) for his significant contributions to the initial phase of this project.

## References

- [CustomYOLOv5-NIST-Buckets-VOXL-tflite](https://github.com/amashry/CustomYOLOv5-NIST-Buckets-VOXL-tflite)
- [AprilTag 3](https://github.com/AprilRobotics/apriltag)
- [Apriltag_ros](http://wiki.ros.org/apriltag_ros)
- [PX4 Autopilot](https://github.com/PX4/PX4-Autopilot)
- [MAVROS](https://github.com/mavlink/mavros)
- [JSON](https://github.com/nlohmann/json)
- [VOXL](https://docs.modalai.com/)
