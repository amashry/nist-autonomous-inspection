# Autonomous Drone Inspection of NIST Buckets
Autonomous search, inspection and detection of National Institute of Standards and Technology (NIST) buckets indoors using ModalAI's VOXL M500 drone.

![BUCKET ALIGNMENT](https://link-to-your-bucket-alignment-image.png)

*Figure 1: Illustration of the GROUND bucket alignment.*

## Overview
This project employs a drone to autonomously inspect buckets in an unknown environment. The buckets are organized in various alignments, each represented by a unique AprilTag. The drone searches a predefined area of interest and upon detecting an AprilTag, it loads the inspection routine corresponding to that tag's bucket alignment. During inspection, the drone utilizes a [custom YOLOv5 model running on TFLITE server](https://github.com/amashry/CustomYOLOv5-NIST-Buckets-VOXL-tflite) onboard on the drone for detecting what's inside the buckets. 

## Introduction
This project's goal was to create software that uses a drone's onboard camera to localize a bucket configuration representing indoor debris in a disaster scenario, aiming to automate the task of a human pilot navigating around these obstacles. By using AprilTag, we simplified the complex task of debris detection into detecting and localizing an AprilTag on the bucket configuration. The drone was then instructed to move to certain predefined poses where its tracking camera could look into the buckets.

A JSON file containing these poses was generated by a program, "generate_waypoints", for each bucket configuration. The localization of the bucket configuration in the local inertial frame was performed using the Pose of the AprilTag in the drone's camera frame and the drone's own pose estimate in the local inertial frame.

By chaining several homogeneous transformations, we obtained the desired pose of the drone in the local inertial frame and fed this information into the drone's autopilot. Smooth drone movement was achieved by controlling yaw rates rather than absolute yaw angles, which reduced image blur from the tracking camera.

This project is primarily built around the ROS (Robot Operating System) framework, and uses MAVROS for interfacing with the PX4 autopilot. The inspection routines are defined as a series of waypoints and movements around the bucket alignments.

## Prerequisites and Dependencies

This project depends on a number of libraries and frameworks:

1. ROS (Robot Operating System): It is the main framework that ties everything together. ROS Melodic was used for this project, but other versions should also work.
2. MAVROS: This is a ROS package that provides a bridge between ROS and the drone's autopilot. It allows us to send commands to the drone and receive telemetry data from it using ROS.
3. Eigen: This is a high-level C++ library for linear algebra, matrix and vector operations, numerical solvers and more. It is used extensively in this project to perform matrix operations and transformations.
4. AprilTag: This is a visual fiducial system popular for robotics research.
5. [JSON for Modern C++](https://github.com/nlohmann/json): A C++ library for reading and writing JSON files. It is used to read the predefined waypoints from JSON files. 

The project also requires a drone equipped with a tracking camera, and an AprilTag attached to the bucket configuration as shown in figure 1 above. 

## Installation and Usage

The installation process begins by cloning the repository and installing the dependencies.

The usage process involves setting up the drone and the ROS environment, launching the ROS node for this project, and then taking off with the drone. Once the drone spots an AprilTag, switch the drone to `offboard` mode, then it will automatically start navigating towards the predefined waypoints. The process can be monitored through the ROS logs or a separate ROS node that visualizes the drone's trajectory.

To use the software, first clone the repository and navigate into it:

```bash
git clone https://github.com/amashry/nist-autonomous-inspection.git
cd nist_autonomous_inspection
```

After installing the dependencies, build the project:

```bash
catkin_make
```
Then, source the workspace's setup file and launch the project using the launch file. 
```bash
source devel/setup.bash
```
You can now launch the ROS node:

```bash
roslaunch launch/indoor.launch
```

## Project Structure

The project is organized in the following directories:

- `src`: Contains the main ROS node (`NFRA_node`) for controlling the drone, and utilities for various tasks like waypoint management, reading bucket configurations, etc.
- `include`: Contains header files for the utilities.
- `launch`: Contains ROS launch files for starting the project.

The `NFRA_node` is the central component of the project, and coordinates the bucket inspection process. It uses MAVROS services and publishes to MAVROS topics to command the drone.

## Significant files
Two main files, "project.cpp" and "utils.cpp", along with a few supporting files and libraries, are responsible for most of the project's implementation. The "project.cpp" file is the main file, whereas "utils.cpp" contains utility functions.

### project.cpp
This file contains the main logic of the software, starting with initializing ROS node, Subscribers, and Publishers. It then reads JSON files containing the waypoint offsets for each bucket. When the drone spots an AprilTag, it loads the corresponding JSON file based on the AprilTag's ID and begins navigating to each waypoint.

For each waypoint, the drone calculates its desired pose in the local inertial frame, which is subsequently published to the "/mavros/setpoint_raw/local" rostopic. The drone continues towards its goal until it approximately reaches the desired position and yaw angle, which is determined using the "drone_is_approximately_at_offset" function from the "utils.cpp" file.

This file also handles the logic for when to transition from one bucket to the next. The desired tolerances are defined as global variables at the top of the "project.cpp" file and can be easily modified to suit the mission requirements.

### utils.cpp
This file contains several utility functions that assist the main program. The "determine_yaw_rate" function calculates the difference in yaw angle between the drone's current pose and its desired pose. This allows us to control the drone's yaw rate for smoother navigation and better image quality.

The "drone_is_approximately_at_offset" function checks if the drone's configuration, defined by its yaw angle and position, is within some predefined tolerance of the desired drone configuration. This helps to determine whether the drone has reached its destination (bucket location) or not.

## Bucket Configurations

Each bucket alignment is represented by a unique AprilTag. The bucket configurations are stored in JSON files inside the `config` directory, and handled by the `BucketConfiguration` class. Each configuration specifies the name of the bucket alignment and an offset matrix.

For example, one of the buckets in the "GROUND" bucket alignment is represented by the following configuration:

```json
{
    "name": "bucket1a",
    "offset": [
        -0.626168, -0.779510, 0.024069, 0.504049, 
        -0.779751, 0.625586, -0.025125, 0.206124, 
        0.004528, -0.034500, -0.999546, -0.42000, 
        0.000000, 0.000000, 0.000000, 1.000000
    ]
}
```

## Mathematical Description

![TRANSFORMATION FRAMES](https://link-to-your-transformation-frames-image.png)

*Figure 2: Illustration of the different reference frames.*

To ensure the drone can navigate accurately around the buckets, the desired pose of the drone is calculated with respect to the AprilTag's frame (which is obtained from the AprilTag detection library). This pose is transformed to the drone's body frame using the equation:

```
pose_body = pose_tag_inverse * pose_desired_tag
```

Finally, the pose in the body frame is transformed to the inertial frame (earth-fixed frame) by multiplying it with the drone's current pose in the inertial frame:

```
pose_inertial = pose_body * pose_current_inertial
```

The pose in the inertial frame is then published and used by the PX4 autopilot for navigation.

## Demonstration Video

![DEMO VIDEO](https://link-to-your-demo-video.png)

*Figure 3: Demonstration video of the autonomous inspection of the "GROUND" bucket alignment by the drone.*

## TODO

- Implement a search routine for the drone to navigate a predefined area of interest.
- Allow the drone to switch between the search and inspection routines dynamically.
- Integrate adaptive path planning for the drone based on the detection. 

## Acknowledgments

A big thank you to [Zach Bortoff](https://github.com/zborffs) for his significant contributions to the initial phase of this project.

## References

- [CustomYOLOv5-NIST-Buckets-VOXL-tflite](https://github.com/amashry/CustomYOLOv5-NIST-Buckets-VOXL-tflite)
- [AprilTag 3](https://github.com/AprilRobotics/apriltag)
- [Apriltag_ros](http://wiki.ros.org/apriltag_ros)
- [PX4 Autopilot](https://github.com/PX4/PX4-Autopilot)
- [MAVROS](https://github.com/mavlink/mavros)
- [JSON](https://github.com/nlohmann/json)
- [VOXL](https://docs.modalai.com/)
